{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 11:51:21.194879: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 11:51:21.194941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 11:51:21.225496: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 11:51:22.709364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random \n",
    "np.random.seed(32)\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "import trax.fastmath.numpy as tnp\n",
    "from trax.supervised import training\n",
    "\n",
    "import pickle\n",
    "from trax import fastmath\n",
    "import random as rnd\n",
    "import os\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'data/'\n",
    "lines = [] # storing all the lines in a variable. \n",
    "for filename in os.listdir(dirname):\n",
    "    with open(os.path.join(dirname, filename)) as files:\n",
    "        for line in files:\n",
    "            # remove leading and trailing whitespace\n",
    "            pure_line = line.strip()\n",
    "            \n",
    "            # if pure_line is not the empty string,\n",
    "            if pure_line:\n",
    "                # append it to the list\n",
    "                lines.append(pure_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125097"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_lines = len(lines)\n",
    "n_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lines = lines[-1000:] # create holdout valid seet\n",
    "lines = lines[:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line, EOS_int=1):\n",
    "    \n",
    "    tensor = []\n",
    "    for c in line:\n",
    "        c_int = ord(c)\n",
    "        tensor.append(c_int)\n",
    "    \n",
    "    tensor.append(EOS_int)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "        batch_size, max_length, data_lines, \n",
    "        line_to_tensor=line_to_tensor, shuffle=True\n",
    "):\n",
    "    \"\"\"Generator function that yields batches of data\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): number of examples per batch.\n",
    "        max_length (int): maximum length of the output tensor.\n",
    "        NOTE: max_length includes the end-of-sentence character that will be added to the tensor. The length of the tensor is always 1 + the length\n",
    "                of the original line of characters.\n",
    "        data_lines (list): list of the sentences to group into batches.\n",
    "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
    "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
    "\n",
    "    Yields:\n",
    "        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).\n",
    "        NOTE: jax.interpreters.xla.DeviceArray is trax's version of numpy.ndarray\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    \n",
    "    cur_batch = []\n",
    "    \n",
    "    num_lines = len(data_lines)\n",
    "    \n",
    "    lines_index = [*range(num_lines)]\n",
    "    \n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if index >= num_lines:\n",
    "            index = 0\n",
    "            if shuffle:\n",
    "                rnd.shuffle(lines_index)\n",
    "            \n",
    "        line = data_lines[lines_index[index]]\n",
    "        \n",
    "        if len(line) < max_length:\n",
    "            cur_batch.append(line)\n",
    "            \n",
    "        index += 1\n",
    "        \n",
    "        # if the current batch is now equal to the desired batch size\n",
    "        if len(cur_batch) == batch_size:\n",
    "            \n",
    "            batch = []\n",
    "            mask = []\n",
    "            \n",
    "            # go through each line in cur_batch\n",
    "            for li in cur_batch:\n",
    "                tensor = line_to_tensor(li)\n",
    "                # how much padding \n",
    "                pad = [0] * (max_length - len(tensor))\n",
    "                \n",
    "                tensor_pad = tensor + pad\n",
    "                \n",
    "                batch.append(tensor_pad)\n",
    "\n",
    "                # A mask for  tensor_pad is 1 wherever tensor_pad is not 0 and 0 wherever tensor_pad is 0\n",
    "                example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
    "                mask.append(example_mask)\n",
    "               \n",
    "            batch_np_arr = np.array(batch)\n",
    "            mask_np_arr = np.array(mask)\n",
    "            \n",
    "            \n",
    "            # yield two copies of the batch and mask.\n",
    "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
    "            \n",
    "            cur_batch = []\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating Batch Generator\n",
    "\n",
    "As the above generator will keep generating batches forever, but `itertools.cycle` is usefull when the generator eventually stops \n",
    "\n",
    "We cycle over the dataset multiple times during training, for small datasets, we can use 'itertools.cycle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden State Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_h = \\left [ W_{hh} \\ | \\ W_{hx} \\right ]$ - horizontal concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 ways to get this:\n",
    "\n",
    "* Method 1 : np.concatenate()\n",
    "* Method 2 : np.hstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 9 9 9]\n",
      " [1 1 9 9 9]\n",
      " [1 1 9 9 9]] \n",
      "\n",
      " [[1 1 9 9 9]\n",
      " [1 1 9 9 9]\n",
      " [1 1 9 9 9]]\n"
     ]
    }
   ],
   "source": [
    "w_hh = np.full((3, 2), 1)\n",
    "w_hx = np.full((3, 3), 9)\n",
    "\n",
    "w_h1 = np.concatenate((w_hh, w_hx), axis=1)\n",
    "w_h2 = np.hstack((w_hh, w_hx))\n",
    "\n",
    "print(w_h1, '\\n\\n', w_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "[h^{<t-1>},x^{<t>}] = \\left[ \\frac{h^{<t-1>}}{x^{<t>}} \\right]\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [9]\n",
      " [9]\n",
      " [9]] \n",
      "\n",
      " [[1]\n",
      " [1]\n",
      " [9]\n",
      " [9]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "h_t_prev = np.full((2, 1), 1)\n",
    "x_t = np.full((3, 1), 9)  \n",
    "\n",
    "ax_1 = np.concatenate((h_t_prev, x_t), axis=0)\n",
    "ax_2 = np.vstack((h_t_prev, x_t))\n",
    "\n",
    "print(ax_1, '\\n\\n', ax_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)                 \n",
    "emb = 128                     \n",
    "T = 256                       \n",
    "h_dim = 16                    \n",
    "h_0 = np.zeros((h_dim, 1))      \n",
    "\n",
    "w1 = random.standard_normal((h_dim, emb+h_dim))\n",
    "w2 = random.standard_normal((h_dim, emb+h_dim))\n",
    "w3 = random.standard_normal((h_dim, emb+h_dim))\n",
    "b1 = random.standard_normal((h_dim, 1))\n",
    "b2 = random.standard_normal((h_dim, 1))\n",
    "b3 = random.standard_normal((h_dim, 1))\n",
    "X = random.standard_normal((T, emb, 1))\n",
    "weights = [w1, w2, w3, b1, b2, b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_V_RNN(inputs, weights): # forward vanilla RNN \n",
    "    x, h_t = inputs\n",
    "    wh, _, _, bh, _, _ = weights\n",
    "\n",
    "    h_t = np.dot(wh, np.concatenate([h_t, x])) + bh\n",
    "    h_t = sigmoid(h_t)\n",
    "\n",
    "    return h_t, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU has relavance and update gate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_GRU(inputs, weights): \n",
    "    x, h_t = inputs\n",
    "    wu, wr, wc, bu, br, bc = weights\n",
    "\n",
    "    u = np.dot(wu, np.concatenate([h_t, x])) + bu\n",
    "    u = sigmoid(u)\n",
    "    \n",
    "    r = np.dot(wr, np.concatenate([h_t, x])) + br\n",
    "    r = sigmoid(u)\n",
    "    \n",
    "    c = np.dot(wc, np.concatenate([r * h_t, x])) + bc\n",
    "    c = np.tanh(c)\n",
    "    \n",
    "    h_t = u* c + (1 - u)* h_t\n",
    "    return h_t, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of the `scan` function**\n",
    "\n",
    "Used in the forward propagation of the RNNs.\n",
    "\n",
    "`scan` goes through all the elements X in elems, calls the function fn with args ([x, h_t], weights), stores the computed hidden h_t and appends the result to a list y_s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(fn, elems, weights, h_0=None):    # Forward prop in RNN\n",
    "    h_t = h_0\n",
    "    ys = []\n",
    "\n",
    "    for x in elems:\n",
    "        y, h_t = fn([x, h_t], weights)\n",
    "        ys.append(y)\n",
    "    \n",
    "    return ys, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 5.02ms to run the forward method for the vanilla RNN.\n"
     ]
    }
   ],
   "source": [
    "# vanilla RNNs\n",
    "tic = perf_counter()\n",
    "ys, h_T = scan(forward_V_RNN, X, weights, h_0)\n",
    "toc = perf_counter()\n",
    "RNN_time=(toc-tic)*1000\n",
    "print (f\"It took {RNN_time:.2f}ms to run the forward method for the vanilla RNN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 16.39ms to run the forward method for the GRU.\n"
     ]
    }
   ],
   "source": [
    "# GRUs\n",
    "tic = perf_counter()\n",
    "ys, h_T = scan(forward_GRU, X, weights, h_0)\n",
    "toc = perf_counter()\n",
    "GRU_time=(toc-tic)*1000\n",
    "print (f\"It took {GRU_time:.2f}ms to run the forward method for the GRU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'\n",
    "vocab_size = 256\n",
    "model_dimension = 512\n",
    "n_layers = 2\n",
    "\n",
    "GRU = tl.Serial(\n",
    "    tl.ShiftRight(mode=mode), # pass the mode parameter if it used for inference and training, \n",
    "    tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n",
    "    [tl.GRU(n_units=model_dimension) for _ in range(n_layers)],\n",
    "    tl.Dense(n_units=vocab_size),\n",
    "    tl.LogSoftmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    ShiftRight(1)\n",
      "  ]\n",
      "  Embedding_256_512\n",
      "  GRU_512\n",
      "  GRU_512\n",
      "  Dense_256\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    \n",
    "    model = tl.Serial(\n",
    "      tl.ShiftRight(mode=mode),\n",
    "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), \n",
    "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], \n",
    "      tl.Dense(n_units=vocab_size), \n",
    "      tl.LogSoftmax() \n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = GRULM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of used lines from the dataset: 25773\n",
      "Batch size (a power of 2): 32\n",
      "Number of steps to cover one epoch: 805\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_length = 64\n",
    "\n",
    "\n",
    "def n_used_lines(lines, max_length):\n",
    " \n",
    "    n_lines = 0\n",
    "    for l in lines:\n",
    "        if len(l) <= max_length:\n",
    "            n_lines += 1\n",
    "    return n_lines\n",
    "\n",
    "\n",
    "\n",
    "num_used_lines = n_used_lines(lines, 32)\n",
    "print('Number of used lines from the dataset:', num_used_lines)\n",
    "print('Batch size (a power of 2):', int(batch_size))\n",
    "steps_per_epoch = int(num_used_lines/batch_size)\n",
    "print('Number of steps to cover one epoch:', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=1, output_dir='model/'): \n",
    "    \n",
    "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
    "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
    "    \n",
    "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
    "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
    "   \n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=infinite_train_generator, \n",
    "        loss_layer=tl.CrossEntropyLoss(),  \n",
    "        optimizer=trax.optimizers.Adam(0.0005)    \n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=infinite_eval_generator,   \n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], \n",
    "        n_eval_batches=3   \n",
    "    )\n",
    "    \n",
    "    training_loop = training.Loop(model,\n",
    "                                  train_task,\n",
    "                                  eval_task=eval_task,\n",
    "                                  output_dir=output_dir)\n",
    "\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    \n",
    "    return training_loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Perplexity\n",
    "\n",
    "Language Model Evaluation\n",
    "Measures how well a prob model predicts a sample\n",
    "\n",
    "$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\n",
    "\n",
    "Taking log on both sides and solving, \n",
    "\n",
    "$$ logP(w)= -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} $$\n",
    "\n",
    "Log of products becomes the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(preds, target):\n",
    "    \"\"\"Function to test the model.\n",
    "\n",
    "    Args:\n",
    "        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.\n",
    "        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.\n",
    "\n",
    "    Returns:\n",
    "        float: log_perplexity of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_p = np.sum(preds * tl.one_hot(target, preds.shape[-1]), axis= -1)\n",
    "\n",
    "    non_pad = 1.0 - np.equal(target, 0)         \n",
    "    log_p = log_p * non_pad                         \n",
    "    \n",
    "    log_ppx = np.sum(log_p, axis=-1) / np.sum(non_pad, axis=-1) \n",
    "    log_ppx = np.mean(log_ppx) \n",
    "    \n",
    "    return -log_ppx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Language with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sample(log_probs, temperature=1.0):\n",
    "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
    "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "    g = -np.log(-np.log(u))\n",
    "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
    "\n",
    "def predict(num_chars, prefix):\n",
    "    inp = [ord(c) for c in prefix]\n",
    "    result = [c for c in prefix]\n",
    "    max_len = len(prefix) + num_chars\n",
    "    for _ in range(num_chars):\n",
    "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
    "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
    "        next_char = gumbel_sample(outp[0, len(inp)])\n",
    "        inp += [int(next_char)]\n",
    "       \n",
    "        if inp[-1] == 1:\n",
    "            break  # EOS\n",
    "        result.append(chr(int(next_char)))\n",
    "    \n",
    "    return \"\".join(result)\n",
    "\n",
    "print(predict(32, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(32, \"\"))\n",
    "print(predict(32, \"\"))\n",
    "print(predict(32, \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
