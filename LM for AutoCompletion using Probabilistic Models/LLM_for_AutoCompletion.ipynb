{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LLM for Auto-Complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "nltk.data.path.append('/home/jerlshin/my_nltk_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jerlshin/nltk_data',\n",
       " '/home/jerlshin/env_ai/nltk_data',\n",
       " '/home/jerlshin/env_ai/share/nltk_data',\n",
       " '/home/jerlshin/env_ai/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data',\n",
       " '/home/jerlshin/my_nltk_data/']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "3335477\n",
      "How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\n",
      "When you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\n",
      "they've decided its more fun if I don't.\n",
      "So Tired D; Played Lazer Tag & Ran A \n"
     ]
    }
   ],
   "source": [
    "with open(\"en_US.twitter.txt\", 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(type(data))\n",
    "print(len(data))\n",
    "\n",
    "print(data[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    sentences = data.split('.\\n')\n",
    "\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def get_tokenized_data(data):\n",
    "    sentences = split_to_sentences(data)\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'am', 'jerlshin'],\n",
       " ['never', 'ever', 'give', 'up'],\n",
       " ['i', 'am', 'the', 'beast'],\n",
       " ['ha', 'ha', 'ha']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"I am Jerlshin.\\nNever Ever give up.\\nI am the beast.\\nHa ha ha.\\n\"\n",
    "get_tokenized_data(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'never']\n",
      "['<s>', 'never', 'ever']\n",
      "['never', 'ever', 'give']\n",
      "['ever', 'give', 'up']\n",
      "['give', 'up', '.']\n",
      "['up', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "'''Trigram'''\n",
    "def sentence_to_n_gram(tokenized_sentence, n):\n",
    "    for i in range(len(tokenized_sentence) -n +1):\n",
    "        trigram = tokenized_sentence[i:i+n]\n",
    "        print(trigram)\n",
    "\n",
    "n = 3 # n-gram\n",
    "tokenized_sentence = ['never', 'ever', 'give', 'up', '.']\n",
    "tokenized_sentence = [\"<s>\"] * (n-1) + tokenized_sentence + [\"</s>\"]\n",
    "\n",
    "sentence_to_n_gram(tokenized_sentence, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenize_sentences):\n",
    "    word_counts = {}\n",
    "\n",
    "    for sentence in tokenize_sentences:\n",
    "        for token in sentence:\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 2,\n",
       " 'am': 2,\n",
       " 'jerlshin': 1,\n",
       " 'never': 1,\n",
       " 'ever': 1,\n",
       " 'give': 1,\n",
       " 'up': 1,\n",
       " 'the': 1,\n",
       " 'beast': 1,\n",
       " 'ha': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = get_tokenized_data(x)\n",
    "count_words(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_test_split(data, train_percent, validation_percent):\n",
    "    # Splits the data \n",
    "    # return train, valid, test data\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    train_size = int(len(data) * train_percent / 100)\n",
    "    train_data = data[0:train_size]\n",
    "\n",
    "    validation_size = int(len(data) * validation_percent / 100)\n",
    "    validation_data = data[train_size:train_size + validation_size]\n",
    "    \n",
    "    test_data = data[train_size + validation_size:]\n",
    "    \n",
    "    return train_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for instance \n",
    "tokenized_data = get_tokenized_data(data)\n",
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11476, 2870)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of Voabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(\n",
    "        tokenized_sentences,\n",
    "        count_threshold\n",
    "):\n",
    "    closed_vocab = []\n",
    "\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "\n",
    "    for word, cnt in word_counts.items():\n",
    "        if cnt >= count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "\n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(\n",
    "        tokenized_sentences, \n",
    "        vocabulary, \n",
    "        unknown_token=\"<unk>\"\n",
    "):\n",
    "    vocabulary = set(vocabulary)\n",
    "    replaced_tokenized_sentences = []\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentences = []\n",
    "        \n",
    "        for token in sentence:\n",
    "            if token in vocabulary:\n",
    "                replaced_sentences.append(token)\n",
    "            else:\n",
    "                replaced_sentences.append(unknown_token)\n",
    "    \n",
    "        replaced_tokenized_sentences.append(replaced_sentences)\n",
    "\n",
    "    return replaced_tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold):\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary)\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary)\n",
    "\n",
    "    return train_data_replaced, test_data_replaced, vocabulary\n",
    "\n",
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing n-gram based Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token='</s>'):\n",
    "    \"\"\"Count all n-grams in the data\n",
    "\n",
    "    Args:\n",
    "        data (list): list of words\n",
    "        n (int): words in seq (order of the gram )\n",
    "        start_token (str, optional)\n",
    "        end_token (str, optional)\n",
    "    \"\"\"\n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = [start_token] * n + sentence + [end_token] # augmentation\n",
    "        sentence = tuple(sentence) # conver the list to tuple so that the seq of words can be used as a key in the dict \n",
    "\n",
    "        '''\n",
    "        i - indicate the start of the n-gram from index 0 to the last index where the end of the n-gram is withing the sentence\n",
    "        '''\n",
    "        m = len(sentence) if n==1 else len(sentence) - 1\n",
    "\n",
    "        for i in range(m): \n",
    "            n_gram = sentence[i:i+n] # n - order of the gram \n",
    "\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1 # increase the count of the n-gram\n",
    "            else:\n",
    "                n_grams[n_gram] = 1 # initialize the n-gram count \n",
    "        \n",
    "    return n_grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "\n",
    "* K-smoothing \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{P}=\\frac{C(w_{n-1}, w_{n})+k}{C(w_{n-1})+k*v}\n",
    "\\end{equation*}\n",
    "\n",
    "* Good-Turing smoothing \n",
    "* Knesser-Ney smoothing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_k_smmoothing_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n",
    "    numerator = n_gram_count + k\n",
    "    denominator = n_gram_prefix_count + k * vocabulary_size\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the probability\n",
    "\n",
    "Estimate the prob of the word given the prior 'n' words using the n-grams counts\n",
    "\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(\n",
    "        word, previous_n_gram,\n",
    "        n_gram_counts,\n",
    "        n_plus1_gram_counts, \n",
    "        vocabulary_size, \n",
    "        k=1.0\n",
    "):\n",
    "    \"\"\"Estimate the prob of the nest word using the n-grams counts with k-smoothing\n",
    "\n",
    "    Args:\n",
    "        word (str): next_word\n",
    "        preprocess_n_gram (str): a seq of words of lenght n\n",
    "        n_gram_counts (dict): dict that maps a tuple of n-words to its freq\n",
    "        n_plus1_gram_counts (dict): countsof n+1 grams \n",
    "        vocabulary_size (int)\n",
    "        k (float, optional): pos const, smoothing parameter. Defaults to 1.0\n",
    "    \"\"\"\n",
    "    # convert to use it as a dict key \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "    # if the prev n-grams exits in the dict of n-grams counts, get its count, else to 0\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    # define n+1 gram as the prev as the n-gram + current word as a tuple\n",
    "    n_plus1_gram = previous_n_gram + (word, ) # for the next gram\n",
    "\n",
    "    # see the next gram if the gram exits there \n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
    "\n",
    "\n",
    "    numerator = n_plus1_gram_count + k\n",
    "\n",
    "    probability = numerator / denominator\n",
    "\n",
    "    return probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Estimating the total probabilities'''\n",
    "\n",
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "\n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    \n",
    "    n_grams = list(set(n_grams))\n",
    "\n",
    "    # making the matrix\n",
    "    row_idex = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
    "    col_idex = {word:j for j, word in enumerate(vocabulary)}\n",
    "\n",
    "    n_row = len(n_grams)\n",
    "    n_col = len(vocabulary)\n",
    "\n",
    "    count_matrix = np.zeros((n_row, n_col))\n",
    "\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1] # excluding the last word \n",
    "        word = n_plus1_gram[-1] # make it as the word \n",
    "\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "\n",
    "        i = row_idex[n_gram]\n",
    "        j = col_idex[word]\n",
    "\n",
    "        count_matrix[i, j] = count\n",
    "\n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back-off\n",
    "- If higher order n-grams prob is missing the lower order n-1 gram is used to get the information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "- Alternative for Backoff. We use weightes prob of n-grams of all orders every time, not just when high order information is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Perplexity**\n",
    "\n",
    "Evaluation score for the language model.\n",
    "\n",
    "- N - lenght of the sentence\n",
    "- n - no of words in n-gram\n",
    "\n",
    "\n",
    "Implementing the m-th order root of a variable\n",
    "\n",
    "\\begin{equation*}\n",
    "PP(W)=\\sqrt[M]{\\prod_{i=1}^{m}{\\frac{1}{P(w_i|w_{i-1})}}}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "As in math, the indexing starts at 0, the range of t changes to t=n to N-1\n",
    "\n",
    "- The more the N-grams tell us about the sentence, the lower the perlexity score will be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of sentences\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of strings\n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of unique words in the vocabulary\n",
    "        k: Positive smoothing constant\n",
    "    \n",
    "    Returns:\n",
    "        Perplexity score\n",
    "    \"\"\"\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    \n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    N = len(sentence)\n",
    "    \n",
    "    product_pi = 1.0\n",
    "    \n",
    "    for t in range(n, N): \n",
    "        n_gram = sentence[t-n:t]\n",
    "        \n",
    "        word = sentence[t]\n",
    "        \n",
    "        probability = estimate_probability(word,n_gram, n_gram_counts, n_plus1_gram_counts, len(unique_words), k=1)\n",
    "\n",
    "        product_pi *= 1 / probability\n",
    "\n",
    "    perplexity = product_pi**(1/float(N))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 3.3674\n",
      "Perplexity for test sample: 3.9654\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "\n",
    "perplexity_train1 = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
    "\n",
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_test = calculate_perplexity(test_sentence,\n",
    "                                       unigram_counts, bigram_counts,\n",
    "                                       len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-Complete System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    \"\"\"\n",
    "    Get suggestion for the next word\n",
    "    \n",
    "    Args:\n",
    "        previous_tokens: The sentence you input where each token is a word. Must have length > n \n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "        start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of \n",
    "          - string of the most likely next word\n",
    "          - corresponding probability\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    \n",
    "    suggestion = None\n",
    "    \n",
    "    max_prob = 0\n",
    "    \n",
    "    for word, prob in probabilities.items(): \n",
    "        if start_with != None: \n",
    "            \n",
    "            if not word.startswith(start_with): \n",
    "                continue  \n",
    "        \n",
    "        if prob > max_prob:\n",
    "            suggestion = word\n",
    "            \n",
    "            max_prob = prob\n",
    "\n",
    "    \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None, top_2=False):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    \n",
    "    if top_2 is True:\n",
    "        return suggestions[:2]\n",
    "    else:\n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like', the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0.2727272727272727),\n",
       " ('a', 0.2),\n",
       " ('dog', 0.1111111111111111),\n",
       " ('dog', 0.1111111111111111)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test your code\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"The previous words are 'i like', the suggestions are:\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'am', 'to'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 0.02807799036074193),\n",
       " ('please', 0.00013528138528138528),\n",
       " ('please', 0.00013533631073216944),\n",
       " ('it', 6.76773145641581e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"am\", \"to\"]\n",
    "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'want', 'to', 'go'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 0.014203478913778),\n",
       " ('to', 0.004581977554950528),\n",
       " ('to', 0.0008780224233418885),\n",
       " ('to', 0.00040565208572780745)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\n",
    "tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('you', 0.023950729927007298),\n",
       " ('you', 0.0041748030435660895),\n",
       " ('you', 0.00013534546931041484),\n",
       " ('it', 6.76773145641581e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\"]\n",
    "tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"'re\", 0.024745032708951283),\n",
       " ('?', 0.0029615004935834156),\n",
       " ('?', 0.0017523758172137225),\n",
       " ('leaving', 0.00013534546931041484)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('do', 0.00837024094916907),\n",
       " ('doing', 0.0017769002961500495),\n",
       " ('doing', 0.00047179348924984834),\n",
       " ('dude', 6.767273465520742e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
    "tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
